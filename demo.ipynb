{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:14:21.214150Z",
     "start_time": "2025-04-10T21:14:21.119376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('hdr_grouping_demo')\n",
    "\n",
    "# Import our pipeline modules\n",
    "from ingest import ImageIngestor\n",
    "from embedding import SceneEmbedder\n",
    "from clustering import SceneClusterer\n",
    "from exposure_sorting import ExposureSorter\n",
    "from validation import ClusterValidator\n",
    "from output import ResultsExporter\n",
    "from utils import estimate_exposure_value\n",
    "\n",
    "# Set matplotlib parameters for better visualization\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "id": "40d31a45de9e643b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Config\n",
    "\n",
    "# HDR Image Grouping Pipeline Configuration Parameters\n",
    "\n",
    "## Ingest Module Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `target_size` | Determines the dimensions (width, height) to which all input images are resized during preprocessing. A value of `(512, 512)` provides a good balance between preserving important details and computational efficiency. Larger sizes preserve more details but require more memory and processing time. |\n",
    "| `max_workers` | Controls the number of parallel processes used during image ingestion. Setting to `16` allows processing multiple images simultaneously, significantly speeding up the ingestion phase for large collections. Should be adjusted based on your CPU's capabilities. |\n",
    "\n",
    "## Embedding Module Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `model_name` | Specifies the deep learning model used to generate image embeddings. `facebook/dinov2-small` is a vision transformer model that produces high-quality representations that are robust to exposure variations. Can be changed to a lighter model if processing speed is a concern. |\n",
    "| `use_gpu` | When `True`, the embedding generation will utilize GPU acceleration if available, which can be significantly faster, especially for deep learning models. |\n",
    "| `use_traditional_features` | When `True`, traditional computer vision features (SIFT/ORB) are used alongside deep learning features, providing complementary information that can improve robustness, especially for extreme exposures. |\n",
    "| `dimensionality_reduction` | Method used to reduce the dimensionality of embeddings. `pca` applies Principal Component Analysis, which preserves major variations while reducing computation for subsequent steps. Other options include `umap` (for better cluster separation) or `None` (no reduction). |\n",
    "| `target_dims` | Target dimensionality for the reduced embeddings. `128` dimensions typically preserve enough information while making clustering more efficient. |\n",
    "| `cache_dir` | Directory where computed embeddings are cached. Using `./embedding_cache` allows faster re-runs as previously processed images won't need to be re-embedded. |\n",
    "| `max_workers` | Number of parallel workers for embedding generation. Set lower than ingest workers (`2`) to avoid GPU memory issues when using deep learning models. |\n",
    "\n",
    "## Clustering Module Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `min_cluster_size` | Minimum number of images required to form a cluster. Setting to `1` allows single-image \"clusters\" which may be useful for unique shots, though usually 2+ is recommended for HDR grouping. |\n",
    "| `min_samples` | Determines how conservative the clustering algorithm is. With `1`, a point can be a core point with just one neighbor, making clustering less restrictive. Higher values create more robust but fewer clusters. |\n",
    "| `cluster_selection_epsilon` | Distance threshold for expanding clusters. `0.1` allows moderately similar points to join existing clusters. Higher values create larger, potentially less coherent clusters. |\n",
    "| `metric` | Distance measure between embeddings. `cosine` is typically best for high-dimensional embeddings as it focuses on the direction rather than magnitude of feature vectors. |\n",
    "| `cluster_selection_method` | Algorithm for extracting flat clusters from the hierarchical structure. `leaf` extracts clusters at leaf nodes, which tends to produce smaller, more numerous clusters compared to the alternative `eom` (Excess of Mass). |\n",
    "\n",
    "## Exposure Sorting Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `ssim_threshold` | Structural Similarity Index threshold for identifying duplicate images. Images with similarity above `0.7` might be considered duplicates. Higher values require greater similarity to flag duplicates. |\n",
    "| `hash_threshold` | Perceptual hash difference threshold for duplicate detection. Images with hash differences below `10` might be duplicates. Lower values are more stringent, requiring greater visual similarity. |\n",
    "| `min_ev_difference` | Minimum exposure value (EV) difference required between images to consider them distinct exposure levels. `0.3` EV represents a noticeable but not dramatic exposure change. Lower values create more granular exposure sequences. |\n",
    "\n",
    "## Validation Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `similarity_threshold` | Minimum average feature similarity required within a cluster for it to be considered valid. `0.7` ensures that images in a cluster share significant visual content. |\n",
    "| `min_hdr_score` | Minimum HDR quality score required for a valid exposure sequence. `0.4` ensures reasonable exposure diversity for HDR purposes. Higher values require more ideal HDR brackets. |\n",
    "| `geometry_threshold` | Minimum geometric consistency score based on homography matching. `0.6` ensures that images in a cluster share the same physical scene structure despite exposure differences. |\n",
    "| `min_cluster_size` | Minimum number of images required for a cluster to be considered valid. `1` allows single-image clusters to be valid, though typically 2+ is better for HDR applications. |\n",
    "| `max_cluster_size` | Maximum images allowed in a single cluster before considering splitting it. `20` prevents overly large clusters that might combine multiple scenes. |\n",
    "\n",
    "## Output Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `create_thumbnails` | When `True`, generates thumbnail previews of all processed images for easier visual inspection. |\n",
    "| `thumbnail_size` | Dimensions (width, height) for generated thumbnails. `(256, 256)` provides a good balance between visibility and file size. |\n",
    "| `export_json` | When `True`, exports detailed metadata about clusters and images in JSON format for potential downstream processing. |\n",
    "| `copy_images` | When `True`, copies the original images to the output directory structure. If `False`, only metadata is exported, which can save disk space. |"
   ],
   "id": "761c6ecec37e9110"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'ingest': {\n",
    "        'target_size': (512, 512),\n",
    "        'max_workers': 16\n",
    "    },\n",
    "    'embedding': {\n",
    "        'model_name': \"facebook/dinov2-small\",  # Can be changed to a lighter model if needed\n",
    "        'use_gpu': True,\n",
    "        'use_traditional_features': True,\n",
    "        'dimensionality_reduction': \"pca\",\n",
    "        'target_dims': 128,\n",
    "        'cache_dir': \"./embedding_cache\",  # Cache embeddings for faster re-runs\n",
    "        'max_workers': 2\n",
    "    },\n",
    "    'clustering': {\n",
    "        'min_cluster_size': 1,\n",
    "        'min_samples': 1,\n",
    "        'cluster_selection_epsilon': 0.1,\n",
    "        'metric': 'cosine',\n",
    "        'cluster_selection_method': 'leaf'\n",
    "    },\n",
    "    'exposure_sorting': {\n",
    "        'ssim_threshold': 0.7,\n",
    "        'hash_threshold': 10,\n",
    "        'min_ev_difference': 0.3\n",
    "    },\n",
    "    'validation': {\n",
    "        'similarity_threshold': 0.7,\n",
    "        'min_hdr_score': 0.4,\n",
    "        'geometry_threshold': 0.6,\n",
    "        'min_cluster_size': 1,\n",
    "        'max_cluster_size': 20\n",
    "    },\n",
    "    'output': {\n",
    "        'create_thumbnails': True,\n",
    "        'thumbnail_size': (256, 256),\n",
    "        'export_json': True,\n",
    "        'copy_images': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Path to the directory containing HDR images\n",
    "input_dir = \"images/shoot_3\"  # Change this to your input directory\n",
    "output_dir = \"output/shoot_3\"  # Change this to your output directory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "id": "a5a1d09f511674ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vizualisation helper functions",
   "id": "11fa39a1fdc2e4b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_image_pair(original, processed, titles=None):\n",
    "    \"\"\"Display an original image beside its processed version.\"\"\"\n",
    "    if titles is None:\n",
    "        titles = [\"Original Image\", \"Processed Image\"]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Convert BGR to RGB for display\n",
    "    if len(original.shape) == 3 and original.shape[2] == 3:\n",
    "        original_rgb = original[..., ::-1]  # BGR to RGB\n",
    "    else:\n",
    "        original_rgb = original\n",
    "\n",
    "    if len(processed.shape) == 3 and processed.shape[2] == 3:\n",
    "        processed_rgb = processed[..., ::-1]  # BGR to RGB\n",
    "    else:\n",
    "        processed_rgb = processed\n",
    "\n",
    "    # Display images\n",
    "    axes[0].imshow(original_rgb, cmap='gray' if len(original.shape) == 2 else None)\n",
    "    axes[0].set_title(titles[0])\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(processed_rgb, cmap='gray' if len(processed.shape) == 2 else None)\n",
    "    axes[1].set_title(titles[1])\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_cluster_samples(clusters, images_data, max_samples=5, max_clusters=5):\n",
    "    \"\"\"Display sample images from each cluster.\"\"\"\n",
    "    # Filter out noise cluster\n",
    "    valid_clusters = {k: v for k, v in clusters.items() if k != -1}\n",
    "\n",
    "    # Limit number of clusters to display\n",
    "    cluster_ids = list(valid_clusters.keys())[:max_clusters]\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        print(f\"\\nCluster {cluster_id} - {len(valid_clusters[cluster_id])} images:\")\n",
    "\n",
    "        # Get random samples from this cluster\n",
    "        cluster_items = valid_clusters[cluster_id]\n",
    "        sample_count = min(max_samples, len(cluster_items))\n",
    "        samples = random.sample(cluster_items, sample_count)\n",
    "\n",
    "        # Create a grid for displaying images\n",
    "        fig, axes = plt.subplots(1, sample_count, figsize=(15, 5))\n",
    "        if sample_count == 1:\n",
    "            axes = [axes]  # Ensure axes is always a list\n",
    "\n",
    "        for i, (img_id, confidence) in enumerate(samples):\n",
    "            img_data = next((img for img in images_data if img['id'] == img_id), None)\n",
    "            if img_data:\n",
    "                # Convert BGR to RGB for display\n",
    "                img_rgb = img_data['original_image'][..., ::-1]\n",
    "                axes[i].imshow(img_rgb)\n",
    "                axes[i].set_title(f\"Confidence: {confidence:.2f}\\nEV: {estimate_exposure_value(img_data['original_image']):.1f}\")\n",
    "                axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def display_exposure_sequence(sequence, images_data):\n",
    "    \"\"\"Display a sorted exposure sequence for a cluster.\"\"\"\n",
    "    # Filter out duplicates and accidental shots\n",
    "    valid_sequence = [info for info in sequence if not info.get('is_duplicate', False) and not info.get('is_accidental', False)]\n",
    "\n",
    "    n_images = len(valid_sequence)\n",
    "    if n_images == 0:\n",
    "        print(\"No valid images in sequence\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
    "    if n_images == 1:\n",
    "        axes = [axes]  # Ensure axes is always a list\n",
    "\n",
    "    for i, info in enumerate(valid_sequence):\n",
    "        img_id = info['id']\n",
    "        img_data = next((img for img in images_data if img['id'] == img_id), None)\n",
    "        if img_data:\n",
    "            # Convert BGR to RGB for display\n",
    "            img_rgb = img_data['original_image'][..., ::-1]\n",
    "            axes[i].imshow(img_rgb)\n",
    "            axes[i].set_title(f\"EV: {info['ev']:.1f}\")\n",
    "            axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "9edffbd4d2ebc7c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline process",
   "id": "890b1187cc5a1a19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Ingest data**",
   "id": "bd16e0cf6ddd315c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Ingesting images from: {input_dir}\")\n",
    "ingestor = ImageIngestor(**config['ingest'])\n",
    "\n",
    "start_time = time.time()\n",
    "images_data = ingestor.ingest_directory(input_dir)\n",
    "ingest_time = time.time() - start_time\n",
    "\n",
    "print(f\"Ingested {len(images_data)} images in {ingest_time:.2f} seconds\")"
   ],
   "id": "7a14f7f9fb57b392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**preview image transforms**",
   "id": "3508e732447aed0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if images_data:\n",
    "    # Select a few random images to display transformations\n",
    "    sample_images = random.sample(images_data, min(3, len(images_data)))\n",
    "\n",
    "    for img_data in sample_images:\n",
    "        print(f\"\\nImage: {img_data['metadata']['filename']}\")\n",
    "        print(f\"Original size: {img_data['metadata']['original_width']}x{img_data['metadata']['original_height']}\")\n",
    "        print(f\"Estimated exposure value: {estimate_exposure_value(img_data['original_image']):.2f}\")\n",
    "\n",
    "        # Display original vs. normalized\n",
    "        display_image_pair(img_data['original_image'], img_data['normalized_image'],\n",
    "                       [\"Original Image\", \"Normalized (Resized) Image\"])\n",
    "\n",
    "        # Display grayscale vs. enhanced contrast\n",
    "        display_image_pair(img_data['grayscale_image'], img_data['enhanced_image'],\n",
    "                       [\"Grayscale Image\", \"Enhanced Contrast (CLAHE)\"])"
   ],
   "id": "534339a7c3342aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create image embeddings",
   "id": "832b56d9f335afe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create cache directory if it doesn't exist\n",
    "os.makedirs(config['embedding']['cache_dir'], exist_ok=True)\n",
    "\n",
    "embedder = SceneEmbedder(**config['embedding'])\n",
    "\n",
    "start_time = time.time()\n",
    "embeddings = embedder.create_embeddings(images_data)\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"Created {len(embeddings)} embeddings in {embedding_time:.2f} seconds\")\n",
    "if len(embeddings) > 0:\n",
    "    sample_id = list(embeddings.keys())[0]\n",
    "    print(f\"Sample embedding shape: {embeddings[sample_id].shape}\")"
   ],
   "id": "c7b726b27f5603b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if len(embeddings) > 2:  # Need at least 3 points for meaningful visualization\n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.vstack(list(embeddings.values()))\n",
    "    image_ids = list(embeddings.keys())\n",
    "\n",
    "    # Use PCA for quick visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "    # Plot PCA\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    plt.title(f\"PCA Visualization of {len(embeddings)} Image Embeddings\")\n",
    "    plt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
    "    plt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # If we have enough samples, also try t-SNE (more computationally intensive)\n",
    "    if len(embeddings) > 5 and len(embeddings) < 1000:  # t-SNE works best with moderate dataset sizes\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(embeddings)-1), max_iter=1000, random_state=42)\n",
    "        tsne_result = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "        # Plot t-SNE\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n",
    "        plt.title(f\"t-SNE Visualization of {len(embeddings)} Image Embeddings\")\n",
    "        plt.xlabel(\"t-SNE Component 1\")\n",
    "        plt.ylabel(\"t-SNE Component 2\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()"
   ],
   "id": "519d8b419d63cec4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Cluster images**",
   "id": "968dab638d4fd737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clusterer = SceneClusterer(**config['clustering'])\n",
    "\n",
    "start_time = time.time()\n",
    "clusters = clusterer.cluster_embeddings(embeddings, images_data)\n",
    "clustering_time = time.time() - start_time\n",
    "\n",
    "# Count valid clusters (excluding noise)\n",
    "valid_cluster_count = len([k for k in clusters.keys() if k != -1])\n",
    "noise_count = len(clusters.get(-1, []))\n",
    "\n",
    "print(f\"Found {valid_cluster_count} clusters in {clustering_time:.2f} seconds\")\n",
    "print(f\"Noise points (unclustered images): {noise_count}\")\n",
    "\n",
    "# Print cluster sizes\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster_id, items in sorted(clusters.items()):\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  Noise cluster: {len(items)} images\")\n",
    "    else:\n",
    "        print(f\"  Cluster {cluster_id}: {len(items)} images\")"
   ],
   "id": "abfd89e80b7ad184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if len(embeddings) > 2 and len(clusters) > 1:  # Need at least 3 points and 2 clusters for meaningful visualization\n",
    "    # Create mapping from image ID to cluster ID\n",
    "    id_to_cluster = {}\n",
    "    for cluster_id, items in clusters.items():\n",
    "        for img_id, _ in items:\n",
    "            id_to_cluster[img_id] = cluster_id\n",
    "\n",
    "    # Create embedding matrix and cluster labels\n",
    "    embedding_matrix = []\n",
    "    cluster_labels = []\n",
    "\n",
    "    for img_id, embedding in embeddings.items():\n",
    "        embedding_matrix.append(embedding)\n",
    "        cluster_labels.append(id_to_cluster.get(img_id, -1))  # Default to noise cluster\n",
    "\n",
    "    embedding_matrix = np.vstack(embedding_matrix)\n",
    "    cluster_labels = np.array(cluster_labels)\n",
    "\n",
    "    # Use PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "    # Plot PCA with cluster colors\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Get unique cluster IDs for coloring\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "    # Create a colormap\n",
    "    import matplotlib.cm as cm\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "    # Plot each cluster with a different color\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        mask = cluster_labels == cluster_id\n",
    "        if cluster_id == -1:\n",
    "            # Plot noise points as black X markers\n",
    "            plt.scatter(pca_result[mask, 0], pca_result[mask, 1], c='black', marker='x', label=f\"Noise\", alpha=0.6)\n",
    "        else:\n",
    "            plt.scatter(pca_result[mask, 0], pca_result[mask, 1], c=[colors[i]], label=f\"Cluster {cluster_id}\", alpha=0.7)\n",
    "\n",
    "    plt.title(f\"PCA Visualization of Clusters ({len(unique_clusters)-1} clusters + noise)\")\n",
    "    plt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
    "    plt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ],
   "id": "7d3ccec29d7bd97e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display up to 5 sample images from each of the top 5 clusters\n",
    "display_cluster_samples(clusters, images_data, max_samples=5, max_clusters=5)"
   ],
   "id": "3b45b51b16451cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Skip if no noise points\n",
    "if -1 in clusters and len(clusters[-1]) > 0:\n",
    "    noise_before = len(clusters.get(-1, []))\n",
    "\n",
    "    start_time = time.time()\n",
    "    updated_clusters = clusterer.assign_outliers(clusters, embeddings, threshold=0.8)\n",
    "    assign_time = time.time() - start_time\n",
    "\n",
    "    noise_after = len(updated_clusters.get(-1, []))\n",
    "    assigned_count = noise_before - noise_after\n",
    "\n",
    "    print(f\"Reassigned {assigned_count} out of {noise_before} noise points in {assign_time:.2f} seconds\")\n",
    "    print(f\"Remaining noise points: {noise_after}\")\n",
    "\n",
    "    # Update our clusters variable\n",
    "    clusters = updated_clusters\n",
    "else:\n",
    "    print(\"No noise points to assign\")"
   ],
   "id": "1b8e3630ef1da41e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cluster_stats = clusterer.get_cluster_statistics(clusters, embeddings)\n",
    "\n",
    "print(\"Cluster Statistics:\")\n",
    "for cluster_id, stats in sorted(cluster_stats.items()):\n",
    "    if cluster_id == -1:\n",
    "        continue  # Skip noise cluster statistics\n",
    "\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Size: {stats['size']} images\")\n",
    "    print(f\"  Average confidence: {stats['avg_confidence']:.4f}\")\n",
    "    print(f\"  Minimum confidence: {stats['min_confidence']:.4f}\")\n",
    "    print(f\"  Coherence: {stats['coherence']:.4f}\")"
   ],
   "id": "b59a9851e2df0aa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dictionary for easier lookup\n",
    "images_dict = {img['id']: img for img in images_data}\n",
    "\n",
    "exposure_sorter = ExposureSorter(**config['exposure_sorting'])\n",
    "\n",
    "# Create a container for processed cluster data\n",
    "processed_clusters = {}\n",
    "\n",
    "print(\"Processing exposure sequences:\")\n",
    "for cluster_id, cluster_items in clusters.items():\n",
    "    # Skip noise cluster\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "\n",
    "    exposure_info = exposure_sorter.process_cluster(cluster_items, images_dict)\n",
    "\n",
    "    # Store processed info\n",
    "    processed_clusters[cluster_id] = {\n",
    "        'items': cluster_items,\n",
    "        'exposure_info': exposure_info\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCluster {cluster_id} - {len(cluster_items)} images:\")\n",
    "    print(f\"  HDR score: {exposure_info['hdr_score']:.4f}\")\n",
    "    print(f\"  Has duplicates: {exposure_info['flags']['has_duplicates']}\")\n",
    "    print(f\"  Has accidental shots: {exposure_info['flags']['has_accidental_shots']}\")\n",
    "\n",
    "    # Get unique exposure values for statistics\n",
    "    evs = [info['ev'] for info in exposure_info['exposure_sequence']]\n",
    "    if evs:\n",
    "        print(f\"  Exposure range: {min(evs):.1f} to {max(evs):.1f} EV (span: {max(evs) - min(evs):.1f})\")\n",
    "\n",
    "    # Show warnings for duplicates and accidental shots\n",
    "    duplicates = [info for info in exposure_info['exposure_sequence'] if info.get('is_duplicate', False)]\n",
    "    accidentals = [info for info in exposure_info['exposure_sequence'] if info.get('is_accidental', False)]\n",
    "\n",
    "    if duplicates:\n",
    "        print(f\"  Found {len(duplicates)} duplicate images\")\n",
    "    if accidentals:\n",
    "        print(f\"  Found {len(accidentals)} accidental shots\")"
   ],
   "id": "9dcda738a205f5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for cluster_id, cluster_data in processed_clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id} - Exposure Sequence:\")\n",
    "    exposure_sequence = cluster_data['exposure_info']['exposure_sequence']\n",
    "\n",
    "    # Display the sorted exposure sequence\n",
    "    display_exposure_sequence(exposure_sequence, images_data)\n",
    "\n",
    "    # Print details about each image in the sequence\n",
    "    print(\"Details:\")\n",
    "    for i, info in enumerate(exposure_sequence):\n",
    "        status = \"\"\n",
    "        if info.get('is_duplicate', False):\n",
    "            status = \"[DUPLICATE]\"\n",
    "        elif info.get('is_accidental', False):\n",
    "            status = \"[ACCIDENTAL]\"\n",
    "\n",
    "        print(f\"  {i+1}. EV: {info['ev']:.1f} {status}\")"
   ],
   "id": "49e65c7da64abbc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validator = ClusterValidator(**config['validation'])\n",
    "\n",
    "print(\"Validating clusters:\")\n",
    "for cluster_id, cluster_data in processed_clusters.items():\n",
    "    cluster_items = cluster_data['items']\n",
    "    exposure_info = cluster_data['exposure_info']\n",
    "\n",
    "    # Validate cluster\n",
    "    validation_result = validator.validate_cluster(\n",
    "        cluster_items,\n",
    "        images_dict,\n",
    "        embeddings,\n",
    "        exposure_info['hdr_score']\n",
    "    )\n",
    "\n",
    "    # Store validation results\n",
    "    processed_clusters[cluster_id]['validation'] = validation_result\n",
    "\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Valid: {validation_result['is_valid']}\")\n",
    "    print(f\"  Confidence: {validation_result['confidence']:.4f}\")\n",
    "\n",
    "    # Print detailed validation scores\n",
    "    details = validation_result['details']\n",
    "    print(\"  Validation details:\")\n",
    "    print(f\"    Feature similarity: {details['similarity_score']:.4f} (valid: {details['similarity_valid']})\")\n",
    "    print(f\"    Geometric consistency: {details['geometry_score']:.4f} (valid: {details['geometry_valid']})\")\n",
    "    print(f\"    HDR score: {details['hdr_score']:.4f} (valid: {details['hdr_valid']})\")\n",
    "    print(f\"    Size: {details['size']} (valid: {details['size_valid']})\")"
   ],
   "id": "6f87a242f9ef447d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for split opportunities\n",
    "split_candidates = {}\n",
    "for cluster_id, cluster_data in processed_clusters.items():\n",
    "    # Check only large or invalid clusters\n",
    "    if len(cluster_data['items']) > config['validation']['min_cluster_size'] * 2 or not cluster_data['validation']['is_valid']:\n",
    "        subclusters = validator.suggest_split(cluster_data['items'], images_dict, embeddings)\n",
    "        if len(subclusters) > 1:\n",
    "            split_candidates[cluster_id] = subclusters\n",
    "\n",
    "# Check for merge opportunities\n",
    "valid_clusters = {\n",
    "    cid: data['items'] for cid, data in processed_clusters.items()\n",
    "    if data['validation']['is_valid']\n",
    "}\n",
    "merge_candidates = validator.suggest_merge(valid_clusters, embeddings)\n",
    "\n",
    "# Report results\n",
    "if split_candidates:\n",
    "    print(\"\\nClusters that could be split:\")\n",
    "    for cluster_id, subclusters in split_candidates.items():\n",
    "        print(f\"  Cluster {cluster_id}: could be split into {len(subclusters)} subclusters\")\n",
    "        for i, sc in enumerate(subclusters):\n",
    "            print(f\"    Subcluster {i+1}: {len(sc)} images\")\n",
    "else:\n",
    "    print(\"\\nNo clusters need splitting\")\n",
    "\n",
    "if merge_candidates:\n",
    "    print(\"\\nClusters that could be merged:\")\n",
    "    for primary_id, secondary_ids in merge_candidates.items():\n",
    "        print(f\"  Cluster {primary_id} could be merged with: {secondary_ids}\")\n",
    "else:\n",
    "    print(\"\\nNo clusters should be merged\")"
   ],
   "id": "e49dc3adc228d3af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the exporter\n",
    "exporter = ResultsExporter(\n",
    "    output_base_dir=output_dir,\n",
    "    **config['output']\n",
    ")\n",
    "\n",
    "# Get valid clusters\n",
    "valid_clusters = {\n",
    "    cid: data for cid, data in processed_clusters.items()\n",
    "    if data['validation']['is_valid']\n",
    "}\n",
    "\n",
    "# Export valid clusters\n",
    "start_time = time.time()\n",
    "export_stats = exporter.export_clusters(valid_clusters, images_dict)\n",
    "export_time = time.time() - start_time\n",
    "\n",
    "print(f\"Exported {export_stats['exported_clusters']} valid clusters in {export_time:.2f} seconds\")\n",
    "print(f\"Exported {export_stats['exported_images']} images\")\n",
    "\n",
    "# Collect unassigned images\n",
    "unassigned_images = []\n",
    "# Add images from noise cluster\n",
    "if -1 in clusters:\n",
    "    unassigned_images.extend([img_id for img_id, _ in clusters[-1]])\n",
    "# Add images from invalid clusters\n",
    "for cid, data in processed_clusters.items():\n",
    "    if not data['validation']['is_valid']:\n",
    "        unassigned_images.extend([img_id for img_id, _ in data['items']])\n",
    "\n",
    "# Export unassigned images\n",
    "start_time = time.time()\n",
    "unassigned_count = exporter.export_unassigned_images(unassigned_images, images_dict)\n",
    "unassigned_time = time.time() - start_time\n",
    "\n",
    "print(f\"Exported {unassigned_count} unassigned images in {unassigned_time:.2f} seconds\")"
   ],
   "id": "4a33b00dd464d308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare pipeline statistics\n",
    "pipeline_stats = {\n",
    "    'total_images': len(images_data),\n",
    "    'ingest_time': ingest_time,\n",
    "    'embedding_time': embedding_time,\n",
    "    'clustering_time': clustering_time,\n",
    "    'validation_time': 0,  # We didn't measure this explicitly\n",
    "    'export_time': export_time + unassigned_time,\n",
    "    'total_time': ingest_time + embedding_time + clustering_time + export_time + unassigned_time\n",
    "}\n",
    "\n",
    "# Generate report\n",
    "exporter.generate_report(export_stats, unassigned_count, pipeline_stats)\n",
    "\n",
    "print(\"\\nFinal Report:\")\n",
    "print(f\"Total images processed: {len(images_data)}\")\n",
    "print(f\"Valid clusters found: {len(valid_clusters)}\")\n",
    "print(f\"Images in valid clusters: {export_stats['total_images']}\")\n",
    "print(f\"Unassigned images: {unassigned_count}\")\n",
    "print(f\"Total processing time: {pipeline_stats['total_time']:.2f} seconds\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")"
   ],
   "id": "372f8b4c6cd53303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Try to load and display the metadata.json file\n",
    "metadata_path = Path(output_dir) / \"grouped_output\" / \"metadata.json\"\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    print(\"Sample from generated metadata.json:\")\n",
    "    print(\"\\nCluster count:\", len(metadata.get('clusters', {})))\n",
    "\n",
    "    # Display info for one random cluster if available\n",
    "    if metadata.get('clusters'):\n",
    "        cluster_id = list(metadata['clusters'].keys())[0]\n",
    "        cluster_meta = metadata['clusters'][cluster_id]\n",
    "\n",
    "        print(f\"\\nSample Cluster {cluster_id}:\")\n",
    "        print(f\"  Confidence: {cluster_meta.get('confidence', 0):.4f}\")\n",
    "        print(f\"  HDR Score: {cluster_meta.get('hdr_score', 0):.4f}\")\n",
    "        print(f\"  Image Count: {cluster_meta.get('image_count', 0)}\")\n",
    "\n",
    "        if cluster_meta.get('images'):\n",
    "            print(\"\\n  First 3 images in sequence:\")\n",
    "            for img in cluster_meta['images'][:3]:\n",
    "                print(f\"    Position {img['sequence_position']} - EV: {img['exposure_value']:.1f} - {Path(img['source_path']).name}\")\n",
    "else:\n",
    "    print(f\"Metadata file not found at {metadata_path}\")"
   ],
   "id": "4e01ccc80bbbc899",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
